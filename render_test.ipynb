{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import copy\n",
    "import time\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "from copy import deepcopy \n",
    "from collections import deque, defaultdict\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from pydantic import dataclasses, validator\n",
    "\n",
    "import dm_control\n",
    "from dm_control import mujoco as dm_mujoco\n",
    "from dm_control.utils.transformations import mat_to_quat, quat_to_euler\n",
    "import mujoco\n",
    "from rocobench.envs import SortOneBlockTask, CabinetTask, MoveRopeTask, SweepTask, MakeSandwichTask, PackGroceryTask, MujocoSimEnv, SimRobot, visualize_voxel_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([-0.207, 2.256, 1.027])-np.array([-0.2, 1.05, 0.1])) # panda cam\n",
    "print(np.array([0.223, -1.064, 0.758]) - np.array([-0.2, -0.07, 0.15])) # other cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "positions = [\n",
    "    (-0.05, 0.5, 0.15),\n",
    "    (-0.25, 0.5, 0.15),\n",
    "    (-0.5, 0.5, 0.15),\n",
    "    (-0.75, 0.5, 0.15),\n",
    "]\n",
    "cameras = []\n",
    "object_cams = []\n",
    "\n",
    "def cross(a, b):\n",
    "    return np.cross(a, b)\n",
    "\n",
    "target = np.array([0, 0, 0])\n",
    "positions = [np.array([0.5, 0, 1])]\n",
    "for position in positions:\n",
    "    vector = target - position\n",
    "    cross_product = cross(vector, np.array([0, 0, 1]))\n",
    "    # Calculate the unit vectors x and y\n",
    "    x = cross_product / np.linalg.norm(cross_product)\n",
    "    x = x/np.linalg.norm(x)\n",
    "    y = np.cross(vector, x)\n",
    "    y = y/np.linalg.norm(y)\n",
    "    cpstring = \" \".join([f\"{i:.3f}\" for i in position])\n",
    "    xystring = \" \".join([f\"{i:.3f}\" for i in y]) + \" \" + \" \".join([f\"{i:.3f}\" for i in x])\n",
    "    print(f\"\"\"<camera mode=\"trackcom\" name='object_ pchdr' pos=\"{cpstring}\" xyaxes=\"{xystring}\"/>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in init loading visual encoder\n",
      "loading physics\n",
      "rendering cameras\n",
      "face_panda\n",
      "face_ur5e\n",
      "teaser\n",
      "sceneshotcam\n",
      "apple\n",
      "milk\n",
      "cereal\n",
      "bread\n",
      "banana\n",
      "bin\n",
      "ur5e\n",
      "panda\n",
      "resetting\n"
     ]
    }
   ],
   "source": [
    "tracked = ['sceneshotcam', 'apple', 'milk', 'cereal', 'bread', 'banana', 'bin', 'ur5e', 'panda']\n",
    "env = PackGroceryTask(\n",
    "    render_freq=2000,\n",
    "    image_hw=(400,400), # Potentially important for getting RGBD images later on\n",
    "    sim_forward_steps=300, # number of time steps forward that mujoco simulates before deciding that the llms need to pick an easier to optimize plan\n",
    "    error_freq=30,\n",
    "    error_threshold=1e-5,\n",
    "    randomize_init=True,\n",
    "    render_point_cloud=0, # Potentially useful for speeding up point fusion\n",
    "    render_cameras=[\"face_panda\",\"face_ur5e\",\"teaser\",],\n",
    "    point_feature_cameras=tracked,\n",
    "    one_obj_each=True, # TODO: Understand this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = env.render_feature_cameras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "test_img = Image.fromarray(outputs['sceneshotcam'][1])\n",
    "test_img.save(\"sceneshotcam.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_BOUNDS=((-1.4, -0.2, -0.1), (1.7, 1.2, 1.1))\n",
    "point_clouds = [\n",
    "    sensor_output[0].point_cloud.filter_bounds(bounds=SCENE_BOUNDS) \n",
    "        for sensor_output in outputs.values()\n",
    "]\n",
    "global_point_cloud = sum(point_clouds[1:], start=point_clouds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "def print_mem_stats():\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_system_memory = mem.total / (1024 ** 2)\n",
    "    used_system_memory = mem.used / (1024 ** 2)\n",
    "    total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)\n",
    "    reserved_gpu_memory = torch.cuda.memory_reserved(0) / (1024 ** 2)\n",
    "    allocated_gpu_memory = torch.cuda.memory_allocated(0) / (1024 ** 2)\n",
    "    percent_gpu_total = (allocated_gpu_memory / total_gpu_memory)*100\n",
    "    percent_gpu_reserved = (reserved_gpu_memory / total_gpu_memory)*100\n",
    "    percent_cpu_total = (used_system_memory / total_system_memory)*100\n",
    "    print(f\"mem used gpu: {allocated_gpu_memory:.2f} MB, reserved gpu: {reserved_gpu_memory:.2f}MB -> {percent_gpu_total:.2f}% of total, {percent_gpu_reserved:.2f}% reserved\")\n",
    "    print(f\"mem used cpu: {used_system_memory:.2f} MB -> {percent_cpu_total:.2f}% of total\")\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "from lavis.models.eva_vit import create_eva_vit_g\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "visual_encoder = create_eva_vit_g(512, precision='fp32').to(device)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "# calculate the global feature vector\n",
    "_, scene_img = outputs[\"sceneshotcam\"]\n",
    "scene_img = cv2.resize(scene_img, (512, 512))\n",
    "scene_tensor = torch.tensor(scene_img[:512,:512]).permute(2, 0, 1)\n",
    "scene_tensor = scene_tensor.unsqueeze(0).float().to(device)\n",
    "output = visual_encoder(scene_tensor)\n",
    "global_feat = output\n",
    "global_feat = global_feat.half().to(device)\n",
    "global_feat = global_feat.mean(1)\n",
    "global_feat = torch.nn.functional.normalize(global_feat, dim=-1)\n",
    "FEAT_DIM = global_feat.shape[-1]\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_features = torch.zeros(global_point_cloud.xyz_pts.shape[0], FEAT_DIM, dtype=torch.half)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_memory_size_in_gb(tensor):\n",
    "    element_size = tensor.element_size()  # Gets size of each element in bytes\n",
    "    total_elements = tensor.numel()        # Gets total number of elements in the tensor\n",
    "    memory_size_in_bytes = element_size * total_elements\n",
    "    memory_size_in_gb = memory_size_in_bytes / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    return memory_size_in_gb\n",
    "\n",
    "print(tensor_memory_size_in_gb(scene_tensor))\n",
    "print(tensor_memory_size_in_gb(global_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "point_feature_cameras = ['sceneshotcam', 'apple', 'milk', 'cereal', 'bread', 'banana', 'bin', 'ur5e', 'panda']\n",
    "specific_views = list(set(point_feature_cameras) - {\"sceneshotcam\"})\n",
    "feat_per_obj = []\n",
    "obj_sim_per_unit_area = []\n",
    "for view in tqdm(specific_views):\n",
    "    # crop the image to the bounding box and run it through the visual encoder to get the feature vector for the object\n",
    "    _, obj_img = outputs[view]\n",
    "    roi = torch.ones((512, 512, 3))\n",
    "    img_roi = torch.tensor(obj_img[:512,:512])\n",
    "    roi[:img_roi.shape[0], :img_roi.shape[1]] = img_roi\n",
    "    img_roi = roi.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    roifeat = visual_encoder(img_roi)\n",
    "    roifeat = roifeat.half().cuda()\n",
    "    roifeat = roifeat.mean(1)\n",
    "    roifeat = torch.nn.functional.normalize(roifeat, dim=-1)\n",
    "    feat_per_obj.append(roifeat)\n",
    "\n",
    "    # calculate the cosine similarity between the global feature vector and the feature vector for the object and save that as well\n",
    "    _sim = cosine_similarity(global_feat, roifeat)\n",
    "    obj_sim_per_unit_area.append(_sim)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(obj_sim_per_unit_area).to(device)\n",
    "feat_per_obj = torch.cat(feat_per_obj, dim=0).to(device)\n",
    "\n",
    "# get the cosine simixlarity between the features of each object. This will be a square matrix where the (i, j)th entry is the cosine similarity between the ith and jth objects\n",
    "mask_sim_mat = torch.nn.functional.cosine_similarity(\n",
    "    feat_per_obj[:, :, None], feat_per_obj.t()[None, :, :]\n",
    ")\n",
    "mask_sim_mat.fill_diagonal_(0.0) # set the diagonal to 0 because we don't want to consider the similarity between the same object\n",
    "mask_sim_mat = mask_sim_mat.mean(1)  # avg sim of each mask with each other mask\n",
    "softmax_scores = scores.cuda() - mask_sim_mat # subtracting the object-object relevance (which can be thought of as the relevance of the object in context of the other objects) object-scene similarity (which is kind of like global relevance) gives how much more or less important that object is than all the other objects\n",
    "softmax_scores = torch.nn.functional.softmax(softmax_scores, dim=0) # apply softmax to get the final scores\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_features = pixelwise_features.to(device)\n",
    "for objidx in range(len(specific_views)):\n",
    "    _weighted_feat = (\n",
    "        softmax_scores[objidx] * global_feat + (1 - softmax_scores[objidx]) * feat_per_obj[objidx]\n",
    "    )\n",
    "    _weighted_feat = torch.nn.functional.normalize(_weighted_feat, dim=-1)\n",
    "    pixelwise_features[global_point_cloud.segmentation_pts[specific_views[objidx]], :] += _weighted_feat\n",
    "    pixelwise_features[global_point_cloud.segmentation_pts[specific_views[objidx]], :] = torch.nn.functional.normalize(\n",
    "        pixelwise_features[global_point_cloud.segmentation_pts[specific_views[objidx]], :],\n",
    "        dim=-1,\n",
    "    ).half()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfeat = pixelwise_features.unsqueeze(0).float().to('cpu')  # interpolate is not implemented for float yet in pytorch\n",
    "xyz_pts = torch.tensor(global_point_cloud.xyz_pts).unsqueeze(0).float()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.common.registry import registry\n",
    "from omegaconf import OmegaConf\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "visual_encoder = None\n",
    "pixelwise_features = None\n",
    "mask_sim_mat = None\n",
    "softmax_scores = None\n",
    "feat_per_obj = None\n",
    "scores = None\n",
    "global_point_cloud = None\n",
    "outputs = None\n",
    "env = None\n",
    "del visual_encoder\n",
    "del pixelwise_features\n",
    "del mask_sim_mat\n",
    "del softmax_scores\n",
    "del feat_per_obj\n",
    "del scores\n",
    "del outputs\n",
    "del env\n",
    "del global_point_cloud\n",
    "gc.collect()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entries = outfeat.size(1)\n",
    "num_to_keep = int(num_entries * 0.6)\n",
    "\n",
    "# Generate a random permutation of indices and select the first 80%\n",
    "indices = torch.randperm(num_entries)[:num_to_keep]\n",
    "\n",
    "# Use the selected indices to downsample the tensors\n",
    "outfeat_downsampled = outfeat[:, indices, :]\n",
    "xyz_pts_downsampled = xyz_pts[:, indices, :]\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfeat = None\n",
    "xyz_pts = None\n",
    "del outfeat\n",
    "del xyz_pts\n",
    "gc.collect()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/pretrain_blip2_sam_flant5xl_v2.pth\"\n",
    "model_cfg = {\n",
    "    \"arch\": \"blip2_t5\",\n",
    "    \"model_type\": \"pretrain_flant5xl\",\n",
    "    \"use_grad_checkpoint\": False,\n",
    "}\n",
    "model_cfg = OmegaConf.create(model_cfg)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = registry.get_model_class(\"blip2_t5\").from_pretrained(model_type=\"pretrain_flant5xl\")\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_cfg = {\"name\": \"blip_question\", \"prompt\": \"\"}\n",
    "processor_cfg = OmegaConf.create(processor_cfg)\n",
    "text_processor = registry.get_processor_class(processor_cfg.name).from_config(processor_cfg)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = text_processor(\"What items do you see on the table?\")\n",
    "model_inputs = {\"text_input\": prompt, \"pc_feat\": outfeat_downsampled, \"pc\": xyz_pts_downsampled}\n",
    "model_outputs = model.predict_answers(\n",
    "    samples=model_inputs,\n",
    "    max_len=50,\n",
    "    length_penalty=1.2,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "model_outputs = model_outputs[0]\n",
    "print(model_outputs)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 21.10 MB, reserved gpu: 42.00MB -> 0.09% of total, 0.19% reserved\n",
      "mem used cpu: 15368.57 MB -> 31.89% of total\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "text_processor = None\n",
    "outfeat_downsampled = None\n",
    "xyz_pts_downsampled = None\n",
    "checkpoint = None\n",
    "del model\n",
    "del text_processor\n",
    "del outfeat_downsampled\n",
    "del xyz_pts_downsampled\n",
    "del checkpoint\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointcloud = env.get_point_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointcloud.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointcloud['cereal'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"rgb_image_sceneshot.png\")\n",
    "img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "focal_length = w / (2 * np.tan(45 * np.pi / 180))\n",
    "K = np.array([[focal_length, 0, w / 2],\n",
    "                  [0, focal_length, h / 2],\n",
    "                  [0, 0, 1]])\n",
    "dist_coeffs = np.zeros(4)\n",
    "new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K, dist_coeffs, (w, h), 1, (w, h))\n",
    "undistorted_img = cv2.undistort(img, K, dist_coeffs, None, new_camera_matrix)\n",
    "x, y, w, h = roi\n",
    "undistorted_img = undistorted_img[y:y+h, x:x+w]\n",
    "cv2.imwrite(\"undistorted_sceneshot.png\", undistorted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"cereal\"\n",
    "minx = min(pointcloud[obj].xyz_pts[:,0])\n",
    "maxx = max(pointcloud[obj].xyz_pts[:,0])\n",
    "miny = min(pointcloud[obj].xyz_pts[:,1])\n",
    "maxy = max(pointcloud[obj].xyz_pts[:,1])\n",
    "minz = min(pointcloud[obj].xyz_pts[:,2])\n",
    "maxz = max(pointcloud[obj].xyz_pts[:,2])\n",
    "width = maxx - minx\n",
    "height = maxy - miny\n",
    "depth = maxz - minz\n",
    "\n",
    "# Find the maximum dimension\n",
    "max_dim = max(width, height, depth)\n",
    "\n",
    "# Calculate the center of the bounding box\n",
    "center_x = (minx + maxx) / 2\n",
    "center_y = (miny + maxy) / 2\n",
    "center_z = (minz + maxz) / 2\n",
    "\n",
    "# Calculate the minimum and maximum coordinates of the bounding box\n",
    "bbox_min = (center_x - max_dim/2, center_y - max_dim/2, center_z - max_dim/2)\n",
    "bbox_max = (center_x + max_dim/2, center_y + max_dim/2, center_z + max_dim/2)\n",
    "bounds = (bbox_min, bbox_max)\n",
    "pointcloud[obj].show(pts_size=50, bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"|{env.model.id2name(48, 'body')}|\")\n",
    "print(env.model.geom(171))\n",
    "print(f\"|{env.model.name2id('apple_visual', 'geom')}|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
