{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eval/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/envs/eval/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import open_clip\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "import psutil\n",
    "from lavis.models.eva_vit import create_eva_vit_g\n",
    "from lavis.common.registry import registry\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 0.00 MB, reserved gpu: 0.00MB -> 0.00% of total, 0.00% reserved\n",
      "mem used cpu: 13147.88 MB -> 27.28% of total\n"
     ]
    }
   ],
   "source": [
    "def print_mem_stats():\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_system_memory = mem.total / (1024 ** 2)\n",
    "    used_system_memory = mem.used / (1024 ** 2)\n",
    "    total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)\n",
    "    reserved_gpu_memory = torch.cuda.memory_reserved(0) / (1024 ** 2)\n",
    "    allocated_gpu_memory = torch.cuda.memory_allocated(0) / (1024 ** 2)\n",
    "    percent_gpu_total = (allocated_gpu_memory / total_gpu_memory)*100\n",
    "    percent_gpu_reserved = (reserved_gpu_memory / total_gpu_memory)*100\n",
    "    percent_cpu_total = (used_system_memory / total_system_memory)*100\n",
    "    print(f\"mem used gpu: {allocated_gpu_memory:.2f} MB, reserved gpu: {reserved_gpu_memory:.2f}MB -> {percent_gpu_total:.2f}% of total, {percent_gpu_reserved:.2f}% reserved\")\n",
    "    print(f\"mem used cpu: {used_system_memory:.2f} MB -> {percent_cpu_total:.2f}% of total\")\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('outputs.pkl', 'rb') as f:\n",
    "    outputs = pickle.load(f)\n",
    "\n",
    "cpu_device = torch.device('cpu')\n",
    "gpu_device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f7dea0e2a34f08be3a28609de5c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 3862.94 MB, reserved gpu: 3884.00MB -> 17.18% of total, 17.27% reserved\n",
      "mem used cpu: 17594.56 MB -> 36.51% of total\n"
     ]
    }
   ],
   "source": [
    "# load visual encoder onto cpu for storage\n",
    "visual_encoder = timm.models.eva_giant_patch14_clip_224(pretrained=True).to(gpu_device)\n",
    "# visual_encoder = create_eva_vit_g(512, precision='fp32').to(gpu_device)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400, 3)\n",
      "mem used gpu: 3872.93 MB, reserved gpu: 3906.00MB -> 17.22% of total, 17.37% reserved\n",
      "mem used cpu: 13801.94 MB -> 28.64% of total\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "# calculate the global feature vector\n",
    "scene_img = outputs[\"sceneshotcam\"]\n",
    "print(scene_img.shape)\n",
    "scene_img = cv2.resize(scene_img, (224, 224))\n",
    "scene_tensor = torch.tensor(scene_img).permute(2, 0, 1).unsqueeze(0).float().to(gpu_device)\n",
    "\n",
    "# use GPU for this\n",
    "global_feat = visual_encoder.forward_features(scene_tensor)\n",
    "global_feat = global_feat.half()\n",
    "global_feat = global_feat.mean(1)\n",
    "global_feat = torch.nn.functional.normalize(global_feat, dim=-1)\n",
    "FEAT_DIM = global_feat.shape[-1]\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global feature shape: torch.Size([1, 1408])\n"
     ]
    }
   ],
   "source": [
    "print(\"global feature shape:\", global_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 3873.53 MB, reserved gpu: 3886.00MB -> 17.22% of total, 17.28% reserved\n",
      "mem used cpu: 13830.04 MB -> 28.70% of total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "point_feature_cameras = ['sceneshotcam', 'apple', 'milk', 'cereal', 'bread', 'banana', 'bin', 'table', 'ur5e', 'panda']\n",
    "specific_views = list(set(point_feature_cameras) - {\"sceneshotcam\"})\n",
    "feat_per_obj = []\n",
    "obj_sim_per_unit_area = []\n",
    "for view in tqdm(specific_views):\n",
    "    # crop the image to the bounding box and run it through the visual encoder to get the feature vector for the object\n",
    "    obj_img = outputs[view]\n",
    "    obj_img = cv2.resize(obj_img, (224, 224))\n",
    "    # obj_img = np.array(Image.open(f\"rocobench/test_images/{view}.png\").resize((400, 400)))\n",
    "    img_roi = torch.tensor(obj_img).permute(2, 0, 1).unsqueeze(0).float().to(gpu_device)\n",
    "    roifeat = visual_encoder.forward_features(img_roi)\n",
    "    roifeat = roifeat.half().cuda()\n",
    "    roifeat = roifeat.mean(1)\n",
    "    roifeat = torch.nn.functional.normalize(roifeat, dim=-1)\n",
    "    feat_per_obj.append(roifeat)\n",
    "\n",
    "    # calculate the cosine similarity between the global feature vector and the feature vector for the object and save that as well\n",
    "    _sim = cosine_similarity(global_feat, roifeat)\n",
    "    obj_sim_per_unit_area.append(_sim)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "visual_encoder = None\n",
    "del visual_encoder\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"global feature shape:\", global_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.9282, 0.9785, 0.9321, 0.9761, 0.9771, 0.9873, 0.9644, 0.9707],\n",
      "        [0.9282, 1.0010, 0.8955, 0.9800, 0.9033, 0.8828, 0.9043, 0.8975, 0.8784],\n",
      "        [0.9785, 0.8955, 0.9995, 0.8940, 0.9717, 0.9956, 0.9932, 0.9526, 0.9937],\n",
      "        [0.9321, 0.9800, 0.8940, 1.0000, 0.8965, 0.8823, 0.9048, 0.8823, 0.8770],\n",
      "        [0.9761, 0.9033, 0.9717, 0.8965, 1.0000, 0.9771, 0.9824, 0.9824, 0.9722],\n",
      "        [0.9771, 0.8828, 0.9956, 0.8823, 0.9771, 1.0010, 0.9927, 0.9536, 0.9956],\n",
      "        [0.9873, 0.9043, 0.9932, 0.9048, 0.9824, 0.9927, 1.0010, 0.9688, 0.9907],\n",
      "        [0.9644, 0.8975, 0.9526, 0.8823, 0.9824, 0.9536, 0.9688, 0.9995, 0.9487],\n",
      "        [0.9707, 0.8784, 0.9937, 0.8770, 0.9722, 0.9956, 0.9907, 0.9487, 1.0000]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "mem used gpu: 3873.54 MB, reserved gpu: 3886.00MB -> 17.22% of total, 17.28% reserved\n",
      "mem used cpu: 13840.43 MB -> 28.72% of total\n",
      "['apple', 'panda', 'bin', 'ur5e', 'bread', 'banana', 'milk', 'cereal', 'table']\n",
      "tensor([0.1119, 0.1177, 0.1096, 0.1190, 0.1079, 0.1086, 0.1095, 0.1074, 0.1085],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.cat(obj_sim_per_unit_area).to(gpu_device)\n",
    "feat_per_obj = torch.cat(feat_per_obj, dim=0).to(gpu_device)\n",
    "# norm_feat_per_obj = torch.nn.functional.normalize(feat_per_obj, p=2, dim=1)\n",
    "\n",
    "# get the cosine simixlarity between the features of each object. This will be a square matrix where the (i, j)th entry is the cosine similarity between the ith and jth objects\n",
    "mask_sim_mat = torch.nn.functional.cosine_similarity(\n",
    "    feat_per_obj[:, :, None], feat_per_obj.t()[None, :, :]\n",
    ")\n",
    "print(mask_sim_mat)\n",
    "mask_sim_mat.fill_diagonal_(0.0) # set the diagonal to 0 because we don't want to consider the similarity between the same object\n",
    "mask_sim_mat = mask_sim_mat.mean(1)  # avg sim of each mask with each other mask\n",
    "softmax_scores = scores.cuda() - mask_sim_mat # subtracting the object-object relevance (which can be thought of as the relevance of the object in context of the other objects) object-scene similarity (which is kind of like global relevance) gives how much more or less important that object is than all the other objects\n",
    "softmax_scores = torch.nn.functional.softmax(softmax_scores, dim=0) # apply softmax to get the final scores\n",
    "print_mem_stats()\n",
    "\n",
    "print(specific_views)\n",
    "print(softmax_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "print(feat_per_obj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1195306, 1408])\n",
      "mem used gpu: 3873.54 MB, reserved gpu: 3886.00MB -> 17.22% of total, 17.28% reserved\n",
      "mem used cpu: 17049.28 MB -> 35.38% of total\n"
     ]
    }
   ],
   "source": [
    "xyz_pts = outputs['xyz_pts']\n",
    "segmentation_pts = outputs['segmentation_pts']\n",
    "\n",
    "pixelwise_features = torch.zeros(xyz_pts.shape[0], FEAT_DIM, dtype=torch.half)#global_feat.repeat((xyz_pts.shape[0], 1))\n",
    "print(pixelwise_features.shape)#torch.zeros(xyz_pts.shape[0], FEAT_DIM, dtype=torch.half)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 7083.59 MB, reserved gpu: 7106.00MB -> 31.49% of total, 31.59% reserved\n",
      "mem used cpu: 13801.87 MB -> 28.64% of total\n"
     ]
    }
   ],
   "source": [
    "pixelwise_features = pixelwise_features.to(gpu_device)\n",
    "for objidx in range(len(specific_views)):\n",
    "    _weighted_feat = (\n",
    "        softmax_scores[objidx] * global_feat + (1 - softmax_scores[objidx]) * feat_per_obj[objidx]\n",
    "    )\n",
    "    _weighted_feat = torch.nn.functional.normalize(_weighted_feat, dim=-1)\n",
    "    pixelwise_features[segmentation_pts[specific_views[objidx]], :] = feat_per_obj[objidx]#_weighted_feat.half()\n",
    "    # pixelwise_features[segmentation_pts[specific_views[objidx]], :] = torch.nn.functional.normalize(\n",
    "    #     pixelwise_features[segmentation_pts[specific_views[objidx]], :],\n",
    "    #     dim=-1,\n",
    "    # ).half()\n",
    "\n",
    "# pixelwise_features = torch.nn.functional.normalize(pixelwise_features, dim=-1)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_row_indices = torch.all(pixelwise_features == 0, dim=1)\n",
    "pixelwise_features = pixelwise_features[~zero_row_indices]\n",
    "xyz_pts = xyz_pts[~zero_row_indices.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1191610, 1408])\n",
      "(1191610, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pixelwise_features.shape)\n",
    "print(xyz_pts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.25 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m pixelwise_features \u001b[38;5;241m=\u001b[39m pixelwise_features\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 1, PCL, feat_dim -> 1, feat_dim, PCL\u001b[39;00m\n\u001b[1;32m      6\u001b[0m xyz_pts \u001b[38;5;241m=\u001b[39m xyz_pts\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m pixelwise_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixelwise_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m xyz_pts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(xyz_pts, [\u001b[38;5;241m15000\u001b[39m], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest-exact\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m pixelwise_features \u001b[38;5;241m=\u001b[39m pixelwise_features\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(cpu_device)\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.8/site-packages/torch/nn/functional.py:4026\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is restricted to bilinear and bicubic modes and requires a 4-D tensor as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_nearest1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest2d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.25 GiB. GPU "
     ]
    }
   ],
   "source": [
    "xyz_pts = torch.tensor(xyz_pts)\n",
    "\n",
    "pixelwise_features = pixelwise_features.unsqueeze(0).float()\n",
    "xyz_pts = xyz_pts.unsqueeze(0).float()\n",
    "pixelwise_features = pixelwise_features.permute(0, 2, 1) # 1, PCL, feat_dim -> 1, feat_dim, PCL\n",
    "xyz_pts = xyz_pts.permute(0, 2, 1)\n",
    "pixelwise_features = torch.nn.functional.interpolate(pixelwise_features, [15000], mode=\"nearest\")\n",
    "xyz_pts = torch.nn.functional.interpolate(xyz_pts, [15000], mode=\"nearest\")\n",
    "pixelwise_features = pixelwise_features.permute(0, 2, 1).to(cpu_device)\n",
    "xyz_pts = xyz_pts.permute(0, 2, 1)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 12.94 MB, reserved gpu: 42.00MB -> 0.06% of total, 0.19% reserved\n",
      "mem used cpu: 4667.71 MB -> 9.69% of total\n"
     ]
    }
   ],
   "source": [
    "visual_encoder = None\n",
    "pixelwise_features = None\n",
    "mask_sim_mat = None\n",
    "softmax_scores = None\n",
    "feat_per_obj = None\n",
    "scores = None\n",
    "outputs = None\n",
    "obj_sim_per_unit_area = None\n",
    "del visual_encoder\n",
    "del pixelwise_features\n",
    "del mask_sim_mat\n",
    "del softmax_scores\n",
    "del feat_per_obj\n",
    "del scores\n",
    "del outputs\n",
    "del obj_sim_per_unit_area\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 12.94 MB, reserved gpu: 42.00MB -> 0.06% of total, 0.19% reserved\n",
      "mem used cpu: 4668.45 MB -> 9.69% of total\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"checkpoints/pretrain_v2.1.pth\"#sqa3d.pth\"#pretrain_v2.1.pth\"\n",
    "model_cfg = {\n",
    "    \"arch\": \"blip2_t5\",\n",
    "    \"model_type\": \"pretrain_flant5xl\",\n",
    "    \"use_grad_checkpoint\": False,\n",
    "}\n",
    "model_cfg = OmegaConf.create(model_cfg)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eval/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Please `pip install timm` to use timm models.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen_clip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m open_clip\u001b[38;5;241m.\u001b[39mlist_pretrained()\n\u001b[0;32m----> 5\u001b[0m model, _, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mopen_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEVA01-g-14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlaion400m_s11b_b41k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEVA01-g-14\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.8/site-packages/open_clip/factory.py:437\u001b[0m, in \u001b[0;36mcreate_model_from_pretrained\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, return_transform, cache_dir, **model_kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_from_pretrained\u001b[39m(\n\u001b[1;32m    418\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    419\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    433\u001b[0m ):\n\u001b[1;32m    434\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[1;32m    435\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[0;32m--> 437\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_pretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_transform:\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.8/site-packages/open_clip/factory.py:250\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         model \u001b[38;5;241m=\u001b[39m CoCa(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_cfg, cast_dtype\u001b[38;5;241m=\u001b[39mcast_dtype)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTextCLIP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     model \u001b[38;5;241m=\u001b[39m CLIP(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_cfg, cast_dtype\u001b[38;5;241m=\u001b[39mcast_dtype)\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.8/site-packages/open_clip/model.py:336\u001b[0m, in \u001b[0;36mCustomTextCLIP.__init__\u001b[0;34m(self, embed_dim, vision_cfg, text_cfg, quick_gelu, init_logit_scale, init_logit_bias, cast_dtype, output_dict)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dict \u001b[38;5;241m=\u001b[39m output_dict\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual \u001b[38;5;241m=\u001b[39m \u001b[43m_build_vision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquick_gelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mcontext_length\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.8/site-packages/open_clip/model.py:119\u001b[0m, in \u001b[0;36m_build_vision_tower\u001b[0;34m(embed_dim, vision_cfg, quick_gelu, cast_dtype)\u001b[0m\n\u001b[1;32m    116\u001b[0m act_layer \u001b[38;5;241m=\u001b[39m QuickGELU \u001b[38;5;28;01mif\u001b[39;00m quick_gelu \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mGELU\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vision_cfg\u001b[38;5;241m.\u001b[39mtimm_model_name:\n\u001b[0;32m--> 119\u001b[0m     visual \u001b[38;5;241m=\u001b[39m \u001b[43mTimmModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_model_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproj_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimm_drop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vision_cfg\u001b[38;5;241m.\u001b[39mlayers, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    132\u001b[0m     vision_heads \u001b[38;5;241m=\u001b[39m vision_cfg\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m vision_cfg\u001b[38;5;241m.\u001b[39mhead_width\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.8/site-packages/open_clip/timm_model.py:47\u001b[0m, in \u001b[0;36mTimmModel.__init__\u001b[0;34m(self, model_name, embed_dim, image_size, pool, proj, proj_bias, drop, drop_path, patch_drop, pretrained)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease `pip install timm` to use timm models.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m to_2tuple(image_size)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# setup kwargs that may not be common across all models\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Please `pip install timm` to use timm models."
     ]
    }
   ],
   "source": [
    "model = registry.get_model_class(\"blip2_t5\").from_pretrained(model_type=\"pretrain_flant5xl\")\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 13.39 MB, reserved gpu: 42.00MB -> 0.06% of total, 0.19% reserved\n",
      "mem used cpu: 24603.96 MB -> 51.06% of total\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 13.39 MB, reserved gpu: 42.00MB -> 0.06% of total, 0.19% reserved\n",
      "mem used cpu: 24607.19 MB -> 51.06% of total\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 13.39 MB, reserved gpu: 42.00MB -> 0.06% of total, 0.19% reserved\n",
      "mem used cpu: 24606.66 MB -> 51.06% of total\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(cpu_device)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 13.39 MB, reserved gpu: 42.00MB -> 0.06% of total, 0.19% reserved\n",
      "mem used cpu: 24607.89 MB -> 51.06% of total\n"
     ]
    }
   ],
   "source": [
    "processor_cfg = {\"name\": \"blip_question\", \"prompt\": \"\"}\n",
    "processor_cfg = OmegaConf.create(processor_cfg)\n",
    "text_processor = registry.get_processor_class(processor_cfg.name).from_config(processor_cfg)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 267174, 1408])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfeat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eval/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.2` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> The door is far away from the window.</s>\n",
      "mem used gpu: 13.39 MB, reserved gpu: 2922.00MB -> 0.06% of total, 12.99% reserved\n",
      "mem used cpu: 24469.65 MB -> 50.78% of total\n"
     ]
    }
   ],
   "source": [
    "# outfeat_downsampled = outfeat_downsampled.to(cpu_device)\n",
    "# xyz_pts_downsampled = xyz_pts_downsampled.to(cpu_device)\n",
    "outfeat = outfeat.to(cpu_device)\n",
    "xyz_pts = xyz_pts.to(cpu_device)\n",
    "prompt = text_processor(\"Describe the 3D scene in one sentence.\")\n",
    "# model_inputs = {\"text_input\": prompt, \"pc_feat\": outfeat_downsampled, \"pc\": xyz_pts_downsampled}\n",
    "model_inputs = {\"text_input\": prompt, \"pc_feat\": outfeat, \"pc\": xyz_pts}\n",
    "model_outputs = model.predict_answers(\n",
    "    samples=model_inputs,\n",
    "    max_len=50,\n",
    "    length_penalty=1.2,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "model_outputs = model_outputs[0]\n",
    "print(model_outputs)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "ax = plt.figure().add_subplot(111, projection=\"3d\")\n",
    "tsne = TSNE(n_components=3, random_state=0, learning_rate=200, init=\"random\")\n",
    "idx = np.random.choice(outfeat.shape[1], 10000)\n",
    "pc_feature = outfeat[:, idx, :]\n",
    "pc_points = xyz_pts[:, idx, :]\n",
    "pc_feature = pc_feature.squeeze(0).cpu().numpy()  # (N, 1408)\n",
    "pc_feature = tsne.fit_transform(pc_feature)  # (N, 3)\n",
    "pc_feature = (pc_feature - pc_feature.min()) / (pc_feature.max() - pc_feature.min() + 1e-6)\n",
    "pc_points = pc_points.squeeze(0).cpu().numpy()  # (N, 3)\n",
    "ax.scatter(pc_points[:, 0], pc_points[:, 1], pc_points[:, 2], c=pc_feature, s=1)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
