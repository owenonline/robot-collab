{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "basic imports\n",
      "local modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/roco3/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/envs/roco3/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import copy\n",
    "import time\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "from copy import deepcopy \n",
    "from collections import deque, defaultdict\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from pydantic import dataclasses, validator\n",
    "\n",
    "import dm_control\n",
    "from dm_control import mujoco as dm_mujoco\n",
    "from dm_control.utils.transformations import mat_to_quat, quat_to_euler\n",
    "import mujoco\n",
    "from rocobench.envs import SortOneBlockTask, CabinetTask, MoveRopeTask, SweepTask, MakeSandwichTask, PackGroceryTask, MujocoSimEnv, SimRobot, visualize_voxel_scene\n",
    "import torch\n",
    "import psutil\n",
    "from lavis.models.eva_vit import create_eva_vit_g\n",
    "from lavis.common.registry import registry\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 0.00 MB, reserved gpu: 0.00MB -> 0.00% of total, 0.00% reserved\n",
      "mem used cpu: 3774.97 MB -> 7.83% of total\n"
     ]
    }
   ],
   "source": [
    "def print_mem_stats():\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_system_memory = mem.total / (1024 ** 2)\n",
    "    used_system_memory = mem.used / (1024 ** 2)\n",
    "    total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)\n",
    "    reserved_gpu_memory = torch.cuda.memory_reserved(0) / (1024 ** 2)\n",
    "    allocated_gpu_memory = torch.cuda.memory_allocated(0) / (1024 ** 2)\n",
    "    percent_gpu_total = (allocated_gpu_memory / total_gpu_memory)*100\n",
    "    percent_gpu_reserved = (reserved_gpu_memory / total_gpu_memory)*100\n",
    "    percent_cpu_total = (used_system_memory / total_system_memory)*100\n",
    "    print(f\"mem used gpu: {allocated_gpu_memory:.2f} MB, reserved gpu: {reserved_gpu_memory:.2f}MB -> {percent_gpu_total:.2f}% of total, {percent_gpu_reserved:.2f}% reserved\")\n",
    "    print(f\"mem used cpu: {used_system_memory:.2f} MB -> {percent_cpu_total:.2f}% of total\")\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in init loading visual encoder\n",
      "loading physics\n",
      "rendering cameras\n",
      "face_panda\n"
     ]
    }
   ],
   "source": [
    "tracked = ['sceneshotcam', 'apple', 'milk', 'cereal', 'bread', 'banana', 'bin', 'ur5e', 'panda']\n",
    "env = PackGroceryTask(\n",
    "    render_freq=2000,\n",
    "    image_hw=(400,400), # Potentially important for getting RGBD images later on\n",
    "    sim_forward_steps=300, # number of time steps forward that mujoco simulates before deciding that the llms need to pick an easier to optimize plan\n",
    "    error_freq=30,\n",
    "    error_threshold=1e-5,\n",
    "    randomize_init=True,\n",
    "    render_point_cloud=0, # Potentially useful for speeding up point fusion\n",
    "    render_cameras=[\"face_panda\",\"face_ur5e\",\"teaser\",],\n",
    "    point_feature_cameras=tracked,\n",
    "    one_obj_each=True, # TODO: Understand this\n",
    ")\n",
    "# gpu_device = torch.device(\"cuda\")\n",
    "# cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 16x16 to 36x36\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cpu_device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load visual encoder onto cpu for storage\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m visual_encoder \u001b[38;5;241m=\u001b[39m create_eva_vit_g(\u001b[38;5;241m512\u001b[39m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mcpu_device\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cpu_device' is not defined"
     ]
    }
   ],
   "source": [
    "# load visual encoder onto cpu for storage\n",
    "visual_encoder = create_eva_vit_g(512, precision='fp32').to(cpu_device)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 3D-LLM onto cpu for storage\n",
    "ckpt_path = \"checkpoints/pretrain_blip2_sam_flant5xl_v2.pth\"\n",
    "model_cfg = {\n",
    "    \"arch\": \"blip2_t5\",\n",
    "    \"model_type\": \"pretrain_flant5xl\",\n",
    "    \"use_grad_checkpoint\": False,\n",
    "}\n",
    "model_cfg = OmegaConf.create(model_cfg)\n",
    "model = registry.get_model_class(\"blip2_t5\").from_pretrained(model_type=\"pretrain_flant5xl\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "model.eval()\n",
    "processor_cfg = {\"name\": \"blip_question\", \"prompt\": \"\"}\n",
    "processor_cfg = OmegaConf.create(processor_cfg)\n",
    "text_processor = registry.get_processor_class(processor_cfg.name).from_config(processor_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = env.render_feature_cameras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.fromarray(outputs['sceneshotcam'][1])\n",
    "test_img.save(\"sceneshotcam.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_BOUNDS=((-1.4, -0.2, -0.1), (1.7, 1.2, 1.1))\n",
    "point_clouds = [\n",
    "    sensor_output[0].point_cloud.filter_bounds(bounds=SCENE_BOUNDS) \n",
    "        for sensor_output in outputs.values()\n",
    "]\n",
    "global_point_cloud = sum(point_clouds[1:], start=point_clouds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "visual_encoder = create_eva_vit_g(512, precision='fp32').to(device)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "# calculate the global feature vector\n",
    "_, scene_img = outputs[\"sceneshotcam\"]\n",
    "scene_img = cv2.resize(scene_img, (512, 512))\n",
    "scene_tensor = torch.tensor(scene_img[:512,:512]).permute(2, 0, 1)\n",
    "scene_tensor = scene_tensor.unsqueeze(0).float().to(device)\n",
    "output = visual_encoder(scene_tensor)\n",
    "global_feat = output\n",
    "global_feat = global_feat.half().to(device)\n",
    "global_feat = global_feat.mean(1)\n",
    "global_feat = torch.nn.functional.normalize(global_feat, dim=-1)\n",
    "FEAT_DIM = global_feat.shape[-1]\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_features = torch.zeros(global_point_cloud.xyz_pts.shape[0], FEAT_DIM, dtype=torch.half)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_memory_size_in_gb(tensor):\n",
    "    element_size = tensor.element_size()  # Gets size of each element in bytes\n",
    "    total_elements = tensor.numel()        # Gets total number of elements in the tensor\n",
    "    memory_size_in_bytes = element_size * total_elements\n",
    "    memory_size_in_gb = memory_size_in_bytes / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    return memory_size_in_gb\n",
    "\n",
    "print(tensor_memory_size_in_gb(scene_tensor))\n",
    "print(tensor_memory_size_in_gb(global_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_feature_cameras = ['sceneshotcam', 'apple', 'milk', 'cereal', 'bread', 'banana', 'bin', 'ur5e', 'panda']\n",
    "specific_views = list(set(point_feature_cameras) - {\"sceneshotcam\"})\n",
    "feat_per_obj = []\n",
    "obj_sim_per_unit_area = []\n",
    "for view in tqdm(specific_views):\n",
    "    # crop the image to the bounding box and run it through the visual encoder to get the feature vector for the object\n",
    "    _, obj_img = outputs[view]\n",
    "    roi = torch.ones((512, 512, 3))\n",
    "    img_roi = torch.tensor(obj_img[:512,:512])\n",
    "    roi[:img_roi.shape[0], :img_roi.shape[1]] = img_roi\n",
    "    img_roi = roi.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    roifeat = visual_encoder(img_roi)\n",
    "    roifeat = roifeat.half().cuda()\n",
    "    roifeat = roifeat.mean(1)\n",
    "    roifeat = torch.nn.functional.normalize(roifeat, dim=-1)\n",
    "    feat_per_obj.append(roifeat)\n",
    "\n",
    "    # calculate the cosine similarity between the global feature vector and the feature vector for the object and save that as well\n",
    "    _sim = cosine_similarity(global_feat, roifeat)\n",
    "    obj_sim_per_unit_area.append(_sim)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cat(obj_sim_per_unit_area).to(device)\n",
    "feat_per_obj = torch.cat(feat_per_obj, dim=0).to(device)\n",
    "\n",
    "# get the cosine simixlarity between the features of each object. This will be a square matrix where the (i, j)th entry is the cosine similarity between the ith and jth objects\n",
    "mask_sim_mat = torch.nn.functional.cosine_similarity(\n",
    "    feat_per_obj[:, :, None], feat_per_obj.t()[None, :, :]\n",
    ")\n",
    "mask_sim_mat.fill_diagonal_(0.0) # set the diagonal to 0 because we don't want to consider the similarity between the same object\n",
    "mask_sim_mat = mask_sim_mat.mean(1)  # avg sim of each mask with each other mask\n",
    "softmax_scores = scores.cuda() - mask_sim_mat # subtracting the object-object relevance (which can be thought of as the relevance of the object in context of the other objects) object-scene similarity (which is kind of like global relevance) gives how much more or less important that object is than all the other objects\n",
    "softmax_scores = torch.nn.functional.softmax(softmax_scores, dim=0) # apply softmax to get the final scores\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_features = pixelwise_features.to(device)\n",
    "for objidx in range(len(specific_views)):\n",
    "    _weighted_feat = (\n",
    "        softmax_scores[objidx] * global_feat + (1 - softmax_scores[objidx]) * feat_per_obj[objidx]\n",
    "    )\n",
    "    _weighted_feat = torch.nn.functional.normalize(_weighted_feat, dim=-1)\n",
    "    pixelwise_features[global_point_cloud.segmentation_pts[specific_views[objidx]], :] += _weighted_feat\n",
    "    pixelwise_features[global_point_cloud.segmentation_pts[specific_views[objidx]], :] = torch.nn.functional.normalize(\n",
    "        pixelwise_features[global_point_cloud.segmentation_pts[specific_views[objidx]], :],\n",
    "        dim=-1,\n",
    "    ).half()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfeat = pixelwise_features.unsqueeze(0).float().to('cpu')  # interpolate is not implemented for float yet in pytorch\n",
    "xyz_pts = torch.tensor(global_point_cloud.xyz_pts).unsqueeze(0).float()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_encoder = None\n",
    "pixelwise_features = None\n",
    "mask_sim_mat = None\n",
    "softmax_scores = None\n",
    "feat_per_obj = None\n",
    "scores = None\n",
    "global_point_cloud = None\n",
    "outputs = None\n",
    "env = None\n",
    "del visual_encoder\n",
    "del pixelwise_features\n",
    "del mask_sim_mat\n",
    "del softmax_scores\n",
    "del feat_per_obj\n",
    "del scores\n",
    "del outputs\n",
    "del env\n",
    "del global_point_cloud\n",
    "gc.collect()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entries = outfeat.size(1)\n",
    "num_to_keep = int(num_entries * 0.6)\n",
    "\n",
    "# Generate a random permutation of indices and select the first 80%\n",
    "indices = torch.randperm(num_entries)[:num_to_keep]\n",
    "\n",
    "# Use the selected indices to downsample the tensors\n",
    "outfeat_downsampled = outfeat[:, indices, :]\n",
    "xyz_pts_downsampled = xyz_pts[:, indices, :]\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfeat = None\n",
    "xyz_pts = None\n",
    "del outfeat\n",
    "del xyz_pts\n",
    "gc.collect()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/pretrain_blip2_sam_flant5xl_v2.pth\"\n",
    "model_cfg = {\n",
    "    \"arch\": \"blip2_t5\",\n",
    "    \"model_type\": \"pretrain_flant5xl\",\n",
    "    \"use_grad_checkpoint\": False,\n",
    "}\n",
    "model_cfg = OmegaConf.create(model_cfg)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = registry.get_model_class(\"blip2_t5\").from_pretrained(model_type=\"pretrain_flant5xl\")\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_cfg = {\"name\": \"blip_question\", \"prompt\": \"\"}\n",
    "processor_cfg = OmegaConf.create(processor_cfg)\n",
    "text_processor = registry.get_processor_class(processor_cfg.name).from_config(processor_cfg)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = text_processor(\"What items do you see on the table?\")\n",
    "model_inputs = {\"text_input\": prompt, \"pc_feat\": outfeat_downsampled, \"pc\": xyz_pts_downsampled}\n",
    "model_outputs = model.predict_answers(\n",
    "    samples=model_inputs,\n",
    "    max_len=50,\n",
    "    length_penalty=1.2,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "model_outputs = model_outputs[0]\n",
    "print(model_outputs)\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem used gpu: 21.10 MB, reserved gpu: 42.00MB -> 0.09% of total, 0.19% reserved\n",
      "mem used cpu: 15368.57 MB -> 31.89% of total\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "text_processor = None\n",
    "outfeat_downsampled = None\n",
    "xyz_pts_downsampled = None\n",
    "checkpoint = None\n",
    "del model\n",
    "del text_processor\n",
    "del outfeat_downsampled\n",
    "del xyz_pts_downsampled\n",
    "del checkpoint\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_mem_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
